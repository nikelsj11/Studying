##Отчет по лабораторной работе №2
&nbsp; | &nbsp;
 --------------------- | ---------------------------------- 
**Предмет**            | *Методы интеллектуального поиска*   
**Выполнил**           | *Красюк Никита*                    
**Группа**             | *НК-401*                           
**Студенческий билет** | *103111130*  
                      
---------------------------------------------------------------------
### Постановка задачи
Реализовать краулер на Python с использованием библиотеки scrapy. Необходимо загрузить максимально возможное количество 
документов и их метаполей:  

&nbsp; | Вариант №4
 ------------------ | ----------------------------------- 
**Cайт**            | habrahabr                           
**Категория**       | Лучшее за месяц                     
**URL**             | http://habrahabr.ru/top/monthly/    
**Метаполя**        | *название поста, имя автора, оценка*

---------------------------------------------------------------------
### Ход решения
Для решения поставленной задачи необходимо создать новый проект scrapy:
```bash
$ scrapy startproject BaseCrawlers 
```
В файле конфигурации определим базовые настройки:
```python2.7
# middleware для установки произвольного useragent, так запросы будут больше похожи на запросы от настоящего человека
DOWNLOADER_MIDDLEWARES = {
    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,
    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
}
# Установка Header: Referer: http://habrahabr.ru/top/monthly/, для тех же целей
DEFAULT_REQUEST_HEADERS = {
    'Referer': 'http://habrahabr.ru/top/monthly/'
}

# в некоторых случаях habr отдает 404 ошибку, в результате проведенных опытов, оказалось, что это связано с отсутствием
# паузы между запросами, устанавливаем паузу = 25 миллисекундам
DOWNLOAD_DELAY = 0.25

# pipeline, позволит обработать полученные значения после парсинга
ITEM_PIPELINES = ['base_crawlers.pipelines.HabrahabrMonthlyItemPipline']
```
Для хранения полученных результатов используем sqlite, т.к. после (или даже во время) сборки данных
можно получить выборки для анализа и визуально оценить качество собираемых данных. Далее необходимо определить краулер, 
в нашем случае это будет: ```base_crawlers.spyders.habrahabr_spyder```. Определим базовые настройки краулера:  
```
name - название используемое для обозначения внутри фрейворка.
allowed_domains - разрешенные для перехода домены.
start_urls - точка входа, с этой страницы начнется обход.
rules - правила в соответствии к которым будет определяться необходимо ли переходить по найденому URL.
```

Общий алгоритм работы можно описать следующим образом:

1. Переходим на url вида ```http://habrahabr.ru/top/monthly/page{номер страницы}/```
2. Находим на странице все url соответствующие запросу: ```//a[@class="button habracut"]/@href``` и последовательно переходим по каждому из них.
3. После каждого запроса передаем результат в функцию парсинга.
4. После парсинга результат идет в pipeline и сохраняется в бд.

После каждого успешного ```код ответа должен быть 200 OK``` перехода по найденному на странице URL, результат
```response``` будет передаваться в функцию для парсинга, определенную в соответствующем правиле, в нашем случае это
функция ```parse_item```. После передачи в функцию объекта ответа, происходит процессинг. Нам необходимо получить
метаполя каждой записи на каждой странице, для этой цели получим список записей на странице, потом пройдемся по каждой
записи внутри цикла и получим необходимые значения. Для этого используем xpath запросы, которые позволяют 
получить определенный список элементов HTML документа, например для получения списка всех уролов ведущих на страницы со
статьями используем слкдующий запрос:  
```python2.7
links = body_selector.xpath('//a[@class="button habracut"]/@href').extract()
```
После процессинга каждой страницы значения передаются в заданный pipeline, для дальнейшей обработки, в нашем случае
для нормализации (приведения к необходимому виду) и сохранению в базу данных.

---------------------------------------------------------------------
### Результаты работы краулера

В результате мы получили базу данных следующего вида:  
```sql
-- Describe HABRAHABR_DATA
CREATE TABLE habrahabr_data (
    title TEXT,
    author CHAR(255),
    rating INTEGER,
    views INTEGER,
    comments INTEGER,
    stars INTEGER,
    tags TEXT,
    article_id INTEGER,
    article_text TEXT NOT NULL
)

```

Для опытов я добавил еще несколько метаполей, например: ```comments, views, tags```. Попробуем
сделать несколько запросов к базе данных. Получим список названий записей отсортированных по рейтингу и
количеству комментариев:
 
```sql
>select title, rating, comments from habrahabr_data  order by -rating, -comments;
```
title                                                                                                 | rating |comments
 ---------------------------------------------------------------------------------------------------- | -----  | ------- 
Как работает реляционная БД | 226 | 131
Реверс-инжиниринг полёта Бэтмена | 224 | 98
История одного факапа Яндекс.Навигатора. В шести действиях с прологом и раскаянием | 156 | 293
Код 15-летней давности и газета объявлений | 155 | 20
19 советов по повседневной работе с Git | 146 | 50
Как я стал программистом. Путь от питерского бездомного до Senior Developer-а за 6 лет | 136 | 72
Самая устаревшая инфраструктура, которую только можно купить за деньги | 121 | 35
Тюним память и сетевой стек в Linux: история перевода высоконагруженных серверов на свежий дистрибутив | 98 | 57
Карта дождей | 97 | 111
Тысяча и один блистер. Поиск лекарств с завышенной ценой | 90 | 158
Зачем нужны почты no-reply@? | 84 | 205
... | ... | ...
 
Либо можно получить список названий статей, тегов и авторов всех записей у которых в тегах есть тег ```Python```.   
 
```sql
>select title, tags, author from habrahabr_data  WHERE tags LIKE  '%Python%';
```

title                                                                     | tags                                                                                       | author  
------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | -----------  
Почему не все так просто с MongoDB | Разработка,Python,NoSQL,MongoDB | @StraNNikk
Унификация дизайна со стороны бэкенда: JavaScript на сервере |  Веб-разработка,Python,Perl,JavaScript,Блог компании Mail.Ru Group |  @bekbulatov
Snaql. Raw SQL в Python-проектах |  SQL,Python |  @artifex
Python 3.5; async/await |  Проектирование и рефакторинг,Программирование,Параллельное программирование,Алгоритмы,Python |  @Alesh
Программа конференции PYCON RUSSIA 2015 |  Программирование,Python,Django,Блог компании IT-People |  @shulyndina
Перенаправление данных из COM-порта в web. Доработка |  Визуализация данных,Веб-разработка,Python,JavaScript,Ajax |  @alguryanow
Python Meetup 28.08.15: полнотекстовый поиск и Europython 2015 |  Поисковые технологии,Python,Open source,Блог компании Wargaming |  @Wargaming
Старый новый pywinauto: автоматизация Windows GUI на Python на примере install/uninstall |  Разработка под Windows,Программирование,Python,Open source,Блог компании Intel  | @vasily-v-ryabov
Разворачиваем Flask-приложение на Nginx, используя Gunicorn |  Python,Nginx |  @VladimirPesterev
Мысли о развёртывании веб-приложений на тестовом сервере  | Серверное администрирование,Веб-разработка,Python,Apache |  @Tanner
Moscow Python Meetup в Rambler&Co |  Программирование,Веб-разработка,Python,Django,Блог компании Rambler&Co |  @defmethod
...|...|... 

В базе данных находятся все записи из категории лучшие за месяц (всего 760 записей). Путь к файлу бд:
*base_crawlers/storage/habrahabr.sqlite*.
  
