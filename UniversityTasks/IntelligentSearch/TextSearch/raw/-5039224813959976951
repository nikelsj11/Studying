#TITLE:Что именно заставляет глубинное обучение и нейронные сети работать хорошо?

      Сейчас очень много статей, рапортующих об успехах нейронных сетей, в частности, в интересующей нас области понимания естественного языка. Но для практической работы важно еще и понимание того, при каких условиях эти алгоритмы не работают, или работают плохо. Отрицательные результаты по понятным причинам часто остаются за рамками публикаций. Часто пишут так — мы использовали метод А вместе с Б и В, и получили результат. А нужен ли был Б и В остается под вопросом. Для разработчика, внедряющего известные методы в практику эти вопросы очень даже важны, поэтому сегодня поговорим об отрицательных результатах и их значении на примерах. Примеры возьмем, как известные, так и из своей практики. 1. Насколько важны объем данных и мощность компьютеров?
Часто говорят, что прогресс в области машинного интеллекта обусловлен в значительной степени ростом производительности компьютеров. В определенной степени это действительно так. Рассмотрим один показательный пример.
Существует такая вещь как модель языка. Модель языка, это условно говоря, функция, способная предсказывать вероятность того, что определенная последовательность слов является допустимой для данного языка. Она же может использоваться для генерации текстов на этом языке. Модель языка используется, например, для коррекции ошибок, которые делает система распознавания речи, что значительно повышает качество результата. 
В 2010 году Томас Миколов продемонстрировал нейронную сеть, реализующую модель языка и превосходящую по своим возможностям традиционные модели на основе n-грамм (новая модель делала на 18% меньше ошибок, чем лучшие из известных средств, что является очень значительным улучшением) (Mikolov et al, 2010). Через три года в 2013 та же нейронная сеть показала лучший результат на задаче распознавания семантических ролей (Mikolov et al, 2013), опередив широко используемый метод условно-случайных полей (CRF). Примерно тогда же было продемонстрировано, что ту же нейросеть можно применить для получения векторного представления слов (когда близким по значению словам сопоставляются близкие вектора). Модификация данного метода легла в основу широко известного ныне word2vec (векторные представления слов были уже известны до этого, но не получали столь широкого распространения, в том числе из-за достаточно сложных и медленных алгоритмов их вычисления). 
Что здесь интересно? То, что использованная модель была впервые разработана Джеффом Элманом аж в 1990 году (Elman J., 1990). На вопрос, что мешало получить указанные результаты еще тогда, обычно отвечают — скорость работы компьютеров и объем обучающей выборки.
Действительно, когда мы сами пытались продублировать результат этих статей и получить вектора слов для русского языка, у нас долго ничего не получалось. Мы проверили все что можно, и уже начали думать, что русский язык сильно особенный и данному методу неподдающийся, но ответ оказался проще. «Красивые» вектора для близких по смыслу слов получаются начиная с определенного объема данных, а до того момента абсолютно ничто не указывает на такую возможность:
Ближайшие соседи для слова «телефон»:50 000 слов
50 0000 слов
2 млн слов
второй
соперников
скачать
кубик
много
использовал
купил
хороший
сотовый
микрофон
сотовый
девайс
трубка
экран

Интересно, что по мере увеличения объема текста, сначала близко оказываются слова не близкие по смыслу, а часто стоящие рядом — на примере видно, что при выборке в 500 000 слов к слову телефон близки те слова, которые часто пишут вместе «использовал телефон», «купил телефон» и так далее. И только на 2 млн. слов мы видим красивую картину синонимов, которую обычно приводят в статьях. На тот момент времени, сей интересный факт нигде не был опубликован, или во всяком случае нам не удалось его найти. Поэтому получить нужный результат помогло упрямство. Отсюда вывод — не всегда нейросетвые модели можно проверить на «простых» или «уменьшенных» случаях, несмотря на огромное желание так сделать. 
Казалось бы все просто. Но возникает вопрос — действительно ли 20 лет развития вычислительной техники были нужны для получения результата? Для опыта мы взяли старый компьютер Pentium-III образца где-то 2000-2002 годов (это был тогда последний писк, но он был) и загрузили его этой задачей. Конечно, вместо часа он работал несколько дней, но получил такой же результат. Так что вопрос тут не только в технике. Можно сказать, что ум разработчика и его вера в используемый метод играют далеко не последнюю роль.
Наш экскурс в историю не претендует на полноту (В 2001 уже были первые нейронные модели языка, но использовали они не сеть Элмана, а скользящее окно, упростили мы и еще ряд моментов), но в целом ситуация показательная и интересная. Очень многие «современные» нейросетевые модели родом из 1990-х (двунаправленные рекуррентные сети — 1997, LSTM – 1998...) и вероятно еще больше интересных разработок терпеливо ждут своего часа. Любопытно, нет ли подобной ситуации и в других областях техники, и какова была бы наша жизнь сейчас если бы эти «бесполезные» изобретения находили дорогу к практике раньше? Впрочем, это уже вопрос из области философии.2. Новые архитектурные решения против объема данных
Вентильные нейронные модули, новые функции активации (RelU — rectified linear unit), maxout, диагональная инициализация, новые архитектуры нейронных сетей, новые способы регуляризации (dropout, dropconnect), и многое другое. Насколько все это играет решающую роль при создании систем анализа текстов? Вот еще один показательный пример.
В 2014 году в работе (Kalchbrenner et al, 2014) cверточные нейронные сети (сама по себе не новая идея), были применены к задаче классификации предложений. Описанная в статье архитектура содержит семь слоев, реализует ряд нестандартных для сверточных сетей методов (dynamic k-max pooling). Вся эта конструкция позволила «поставить новый рекорд точности» в предсказании тональности предложений из отзывов о фильмах, и в классификации типов вопросов (важная задача для систем, способных отвечать на вопросы, задаваемые пользователем на естественном языке). Бесспорное достижение. Через несколько месяцев после появления этой статьи, мы обнаружили новую статью (Kim et al, 2014), в которой показаны еще более улучшенные результаты. Но сверточная сеть в этой работе содержит всего три слоя, не использует k-max pooling и вообще имеет самое простое строение. 
Работу (Kim et al, 2014) мы попробовали воспроизвести, и очень долго ничего не получалось, причем катастрофически (на одном из наборов данных выходило около 65% точности против опубликованных 81%). Причина оказалась опять простой — использование других векторов слов. Первоначально мы взяли вектора, обученные на корпусе из 400 миллионов слов. Казалось бы — много. Но в оригинале использовались вектора word2vec которые тренированы на 100 миллиардах слов. Разница. Взяв вектора натренированные на 5 млрд. слов сразу получили 78%. Без других усилий. Более того, выяснилось, что набор векторов имеет большее значение, чем архитектура нейросети. Если вектора слов, полученные на 5 млрд слов дать простой модели NBOW (neural bag of words), то получается 69-72%. Вышло, что опять существенную роль играет объем данных.
Из приведенных примеров может показаться, что можно сколь угодно совершенствовать архитектуру системы, но если нет больших объемов данных, то это бесполезно. Но однако, это не всегда так. Возьмем, например, задачу распознавания именованных сущностный, таких как имена людей или названия организаций. Классические ныне методы, такие как CRF с массой созданных вручную признаков на огромном числе примеров (миллионы аннотированных слов), решают такую задачу легко, выдавая F1 0.89-0.92. Но если нет миллионов аннотированных слов и ручных признаков, то что делать? Однажды нам понадобилось быстро решить подобную задачу. Вручную за неделю мы сделали небольшую (порядка 100К слов) аннотированную выборку. Придумывать признаки или размечать больше данных времени и возможности не было. Стандартная двунаправленная рекуррентная сеть Элмана с векторами слов, обученными на данных Википедии при тестировании дала не особо выдающиеся 77%. Тогда мы решили архитектуру наворотить, сделали много слоев, применили функцию активации RelU, особый способ инициализации весов и рекуррентные связи от верхних слоев к нижним. Результат поднялся до приличных 86.7%. В данном случае новые архитектурные решения сыграли решающую роль. 
Все это конечно частные примеры, которые не претендуют на какие-то глобальные обобщения и выводы, но представляются интересными в плане понимания того, какие факторы действительно играют роль в практической работе с нейронными сетями. На этом мы пока завершим тему, но если будет к ней интерес, то продолжим в следующих статьях. Краткий список литературы
1.Tomas Mikolov, Martin Karafiat´, Luka´s Burget, Jan Cernock, Sanjeev Khudanpur. Recurrent neural network based language model//In: Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010), Makuhari, Chiba, JP
2. Yao, Kaisheng, Geoffrey Zweig, Mei-Yuh Hwang, Yangyang Shi, and Dong Yu. «Recurrent neural networks for language understanding.» In INTERSPEECH, pp. 2524-2528. 2013.
3. Jeffrey L. Elman. Finding Structure in Time. Cognitive Science,14, 179-211
4. Kalchbrenner, Nal, Edward Grefenstette, and Phil Blunsom. «A convolutional neural network for modelling sentences.» arXiv preprint arXiv:1404.2188 (2014).
5. Kim, Yoon. «Convolutional neural networks for sentence classification.» arXiv preprint arXiv:1408.5882 (2014).

      
      
    